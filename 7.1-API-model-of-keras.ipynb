{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수형 API 소개\n",
    "`함수형 API`에서는 직접 텐서들의 입출력을 다룹니다. 함수처럼 층을 사용하여 텐서를 입력받고 출력합니다(그래서 함수형 API라고 부릅니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jinlongbin_lab\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "input_tensor = Input(shape=(32, ))\n",
    "dense = layers.Dense(32, activation='relu')\n",
    "output_tensor = dense(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential 모델과 함수형 API로 만든 동일한 모델을 나란히 비교해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 객체를 사용한 컴파일, 훈련, ,평가 API는 Sequential 클래스와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jinlongbin_lab\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 300us/step - loss: 11.6392\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.6027\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5938\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5872\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.5827\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.5788\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.5765\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 11.5743\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 11.5715\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 11.5696\n",
      "1000/1000 [==============================] - 0s 45us/step\n",
      "11.566178436279296\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000,64))\n",
    "y_train = np.random.random((1000,10))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "score = model.evaluate(x_train, y_train)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력 모델\n",
    "함수형 API는 다중 입력 모델을 만드는 데 사용할 수 있습니다. 일반적으로 이런 모델은 서로 다른 입력 가지를 합치기 위해 여러 텐서를 연겷할 수 있는 층을 사용합니다. 텐서를 더하거나 이어 붙이는 식입니다. 이와 관련된 케라스 함수는 `keras.layers.add`, `keras.layers.concatenate`등입니다. 아주 간단한 다중 입력 모델을 살펴보겠습니다. 질문-응답(question-answering) 모델입니다.\n",
    "\n",
    "전형적인 질문-응답 모델은 2개의 입력을 가집니다. 하나는 자연어 질문이고, 또 하나는 답변에 필요한 정보가 담겨있는 텍스트(예를 들어 뉴스 기사)입니다. 그러면 모텔은 답을 출력해야 합니다. 가장 간단한 구조는 미리 정의한 어휘 사전에서 소프트맥스 함수를 통해 한 단어로 된 답을 출력하는 것입니다.\n",
    "\n",
    "다은은 함수형 API를 사용하여 이런 모델을 만드는 예입니다. 텍스트와 질문을 벡터로 인코딩하여 독립된 입력 2개를 정의합니다. 그다음 이 벡터를 결하고 그 위에 소프트맥스 분류기를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 64)     640000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, None, 32)     320000      question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 32)           12416       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 32)           8320        embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64)           0           lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 500)          32500       concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,013,236\n",
      "Trainable params: 1,013,236\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# 텍스트 입력은 길이가 정해지지 않은 정수 시퀀스입니다. 입력 이름을 지정할 수 있습니다.\n",
    "text_input = Input(shape=(None, ), dtype='int32', name='text')\n",
    "# 입력을 크기가 64인 벡터의 시퀀스로 임베딩합니다.\n",
    "embedded_text = layers.Embedding(text_vocabulary_size, 64)(text_input)\n",
    "# LSTM을 사용하여 이 벡터들을 하나의 벡터로 인코딩 합니다.\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "question_input = Input(shape=(None, ), dtype='int32', name='question')\n",
    "embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)\n",
    "encoded_question = layers.LSTM(32)(embedded_question)\n",
    "\n",
    "# 인코딩된 질문과 텍스트를 연결합니다.\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "# 소프트맥스 분류기를 추가합니다.\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# 모델 객체를 만들고 2개의 입력과 출력을 주입합니다.\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 이렇게 입력이 2개인 모델은 어떻게 훈렪할까요? 두가지 방식이 있습니다.. 넘파이 배열의 리스트를 주입하거나 입력 이름과 넘파이 배열로 이루어진 데셔너리를 모델의 입력으로 주입할 수 있습니다. 당연하세 두번째 방식은 입력 이름을 설정했을 때 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 6.2145 - acc: 1.0000e-03\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 6.1987 - acc: 0.0530\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 6.1455 - acc: 0.0240\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 930us/step - loss: 6.0384 - acc: 0.0050\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 5.9692 - acc: 0.0060\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 937us/step - loss: 5.8987 - acc: 0.0080\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 5.8174 - acc: 0.0110\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 988us/step - loss: 5.7365 - acc: 0.0110\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 947us/step - loss: 5.6726 - acc: 0.0140\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 5.5703 - acc: 0.0250\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 5.5118 - acc: 0.0260\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 5.4399 - acc: 0.0360\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 5.3894 - acc: 0.0360\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 5.2873 - acc: 0.0460\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 5.2136 - acc: 0.0470\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 5.1771 - acc: 0.0600\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 5.1069 - acc: 0.0650\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 5.0414 - acc: 0.0750\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 952us/step - loss: 4.9930 - acc: 0.0880\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 947us/step - loss: 4.9337 - acc: 0.0770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x239e32b87b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "# 랜덤한 넘파이 배열을 생성합니다.\n",
    "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
    "\n",
    "answers = np.random.randint(0, answer_vocabulary_size, size=num_samples)\n",
    "# 답은 정수가 아닌 원-핫 인코딩 된 벡터입니다.\n",
    "answers = to_categorical(answers)\n",
    "\n",
    "# 리스트 입력을 사용하여 학습합니다.\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)\n",
    "# 딕셔너리 입력을 사용하여 학습합니다(입력 이름을 지정했을 때만 사용할 수 있습니다.)\n",
    "model.fit({'text':text, 'question':question}, answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 출력 모델\n",
    "같은 식으로 함수형 API를 사용하여 다중 출력(또는 다중 머리) 모델을 만들 수 있습니다. 간단한 예는 데이터에 있는 여러 속성을 동시에 예측하는 네트워크입니다. 예를 들어 소셜 미디어에서 익명 사용자의 포스트를 입력으로 받아 그 사람의 나이 성별 소득 수준 등을 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(vocabulary_size, 256)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# 출력층의 이름을 저장합니다\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 모델을 훈련하려면 네트워크 출력마다 다른 손실 함수를 지정해야 합니다. 예를 들어 나이 예측은 스칼라 회귀 문제이지만 성별 예측은 이진 클래스 문제라 훈련 방식이 다릅니다. 경사 하강법은 하나의 스칼라 값을 최소화하기 때문에 모델을 훈련하려면 이 손실들을 하나의 값으로 합쳐야 합니다. 손실 값을 합치는 가장 간단한 방법은 모두 더하는 것입니다. 케라스에서는 compile 메서드에 리스트나 딕셔너리를 사용하여 출력마다 다른 손실을 지정할 수 있습니다. 계산된 손실값은 전체 손실 하나로 더해지고 훈련 과정을 통해 최소화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "# 위와 동일합니다(출력 층에 이름을 지정했을 때만 사용할 수 있습니다.)\n",
    "model.compile(optimizer='rmsprop', loss={'age':'mse', \n",
    "                                         'income':'categorical_crossentropy', \n",
    "                                         'gender':'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 값이 많이 불균형하면 모델이 개별 손실이 가장 큰 작업에 치우쳐 표현을 최적화할 것입니다. 그 결과 다른 작업들은 손해를 입습니다. 이를 해결하기 위해 손실값이 최종 손실에 기여하는 수준을 지정할 수 있습니다. 특히 손실 값의 스케일이 다를 때 유용합니다. 예를 들어 다음과 같이 가정해 보죠. 나이 회귀 작업에 사용되는 평균 제곱 오차(MSE) 손슬은 일반적으로 3~5 사잉의 값을 가집니다. 반면에 성별 분류 작업에 사용되는 크로스엔트로피 손실은 0.1 정도로 낮습니다. 이런 환경에서 손실에 균형을 맞추려면 크로스엔트로피 손실에 가중치 10을 주고 MSR 손실에 가중치 0.25를 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['mse','categorical_crossentropy','binary_crossentropy'], loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "# 위와 동일합니다(출력 층에 이름을 지정했을 때만 사용할 수 있습니다.)\n",
    "model.compile(optimizer='rmsprop', loss={'age':'mse', 'income':'categorical_crossentropy', 'gender':'binary_crossentropy'},\n",
    "                                   loss_weights={'age':0.25, 'income':1., 'gender':10.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# age_targets, income_targets, gender_targets가 넘파이 배열이라고 가정합니다.\n",
    "model.fit(posts_input, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)\n",
    "\n",
    "model.fit(posts_input, {'age':age_targets, 'income':income_targets, 'gender':gender_targets}, epochs=10, batch_size=64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 층으로 구성된 비순환 유향 그래프\n",
    "함수형 API를 사용하면 다중 입력이나 다중 출력 모델 뿐만 아니라 내부 토폴로지가 복잡한 네트워크도 만들 수 있습니다. 케라스의 신경망은 층으로 구성된 어떤 `비순환 유향 그래프`(directed acyclic graph)도 만들 수 있습니다. 비순환이라는 것이 중요합니닫. 다시 말해 이 그래프는 원형을 띨 수 없습니다. 텐서 ㅌ가 자기 자신을 출력하는 층의 입력이 될 수 없습니다. 만들 수 있는 루프(즉 순호 연결)는 순환 층의 내부에 있는 것뿐입니다.\n",
    "\n",
    "그래프로 구현된 몇 개의 신경망 컴포넌트가 널리 사용됩니다. 가장 유명한 2개는 인셉션 모듈과 잔차 연결입니다. 케라스에서 이 2개의 컴포넌트를 어떻게 구현하는지 살펴보겠습니다. 함수형 API를 사용하여 층의 그래프를 만드는 방법을 이해 하는 데 도움이 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인셉션 모듈\n",
    "`인셉션(Inception)`은 합성곱 신경망에서 인기있는 네트워크 구조입니다. 일찍이 너트워크 안의 네트워크(network-in-network) 구조에서 영감을 받아 2013~2014년에 크리스티안 세게디(Christian Szegedy)와 그의 구글 동료들이 만들었습니다. 나란히 분리된 가지를 따라 모듈을 쌓아 독립된 작은 네트워크처럼 구성합니다. 가장 기본적인 인셉션 모듈 형태는 3~4개의 가지를 가집니다. 1x1합성곱으로 시작해서 3x3 합성곱이 뒤따르고 마지막에 전체 출력 특성이 합쳐집니다. 이런 구성은 네트워크가 따로따로 공간 특성과 채널 방향의 특성을 학습하도록 돕습니다. 한 꺼번에 학습하는 것보다 효과가 더 높습니다. 더 복잡한 인셉션 모듈은 풀링 연산, 여러 가지 합성곱 사이즈(예를 들어 일부 가지에서는 3x3대신 5x5를 사용합니다), 공간 합성곱이 없는 가지(1x1 합성곱만 있습니다)로 구성될 수 있습니다. 다음은 함수형 API를 사용하여 `인셉션 V3(Inception V3)`모듈을 구현하는 예입니다. 이 예에서 입력 x는 4D텐서라고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 128, 128, 128 512         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 128, 128, 128 512         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 64, 64, 3)    0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 128, 128, 128 147584      conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 64, 128)  512         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 64, 128)  3584        average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 512)  0           conv2d_51[0][0]                  \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 447,872\n",
      "Trainable params: 447,872\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=(128,128, 3))\n",
    "# 모든 가지는 동일한 스트라이드(2)를 사용합니다. 출력 크기를 동일하게 만들어 하나로 합치기 위해서입니다.\n",
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x)\n",
    "\n",
    "# 이 가지에서는 두번째 합성곱 층에서 스트라이드를 적용합니다.\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu', padding='same')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_b)\n",
    "\n",
    "# 이 가지에서는 평균 풀링 층에서 스트라이드를 적용합니다.\n",
    "branch_c = layers.AveragePooling2D(3, strides=2, padding='same')(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu', padding='same')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_d)\n",
    "\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)\n",
    "model = Model(x, output)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` 1x1 합성곱의 목적\n",
    "\n",
    "이미 알고 있듯이 합성곱은 입력 텐서에서 타일 주변의 패치를 추출하고 각 패치에 동일한 연산을 수행합니다. 이 경유는 추출된 패치가 하나의 타일로 이루어졌을 때입니다. 이 합성곱 연산은 모든 타일 벡터를 하나의 Dense 층에 통과시키는 것과 동일합니다. 즉 입력 텐서의 채널 정보를 혼합한 특성을 계산합니다. 공간 방향으로는 정보를 섞지 않습니다(한번에 하나의 타일만 처리하기 때문입니다). 이런 1x1 합성곱(또는 점별 합성곱(pointwise convolution))은 인셉션 모듈의 특징입니다. 채널 방향의 특성 학습과 공간 방향의 특성 학습을 분리하는 데 도움을 줍니다. 채널이 공간 방향으로 상관관계가 크고 채널 간에는 독립적이라고 가정하면 납득할 만한 전략입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인셉션 V3 전체 구조는 케라스의 keras.applications.inception_v3.InceptionV3에 준비되어 있으며, ImageNet 데이터셋에서 사전 훈련된 가중치를 포함하고 있습니다. 이와 아주 비슷한 모델인 `엑셉션(Xception)`도 케라스의 애플리케이션 모듈에 포함되어 있습니다. 엑셉션은 극단적인 인셉션(extreme inception)을 말합니다. 이 합성곱 구조는 인셉션에서 일부 영감을 얻었습니다. 채널 방향의 학습과 공간방향의 학습을 극잔적으로 분리한다는 아이디어에 착안하여 인셉션 모듈을 깊이별 분리 합성곱으로 바꿉니다. 이 합성곱은 깊이별 합성곱(depthwise convolution)(각 입력 채널에 따로따로 적용되는 공간 방향 합성곱) 다음에 점별 합성곱(1x1 합성곱)이 뒤따릅니다. 인셉션 모듈의 극한 형태로 공간 특성과 채널 방향 특성을 완전히 분리합니다. 엑셉션은 인셉션V3와 거의 동일한 개수의 모델 파라미터를 가지지만 실행 속도가 더 빠르고 ImageNet이나 다른 대규모 데이터셋에서 정확도가 더 높습니다. 이는 모델 파라미터를 더 효율적으로 사용하기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잔차 연결\n",
    "`잔차 연결`(residual connection)은 엑셉션을 포함하여 2015년 이후 등장한 많은 네트워크 구조에 있는 그래프 형태의 네트워크 컴퓨넌트입니다. 2015년 후반 ILSVRC ImageNet 경연 대회 우승 팀인 마이크로소프트의 허 등이 소개했습니다. 대규모 딥러닝 모델ㅇ에서 흔히 나타나는 두가지 문제인 그래디언트 소싥과 표현 병목(representational bottleneck)을 해결했습니다. 일반적으로 10개 층 이상을 가진 모델에 잔차 연결을 추가하면 도움이 됩니다.\n",
    "\n",
    "잔차 연결은 하위 층의 출력을 상위층의 입력으로 사용합니다. 순서대로 놓인 네트워크를 질러가는 연결이 만들어집니다. 하위 층의 출력이 상의 층의 활성화 출력에 연결괴는 것이 아니고 더해집니다. 따라서 두 출력의 크기가 동일해야 합니다. 크기가 다르면 선형변환을 사용하여 하의층의 활성화 출력을 목표 크기로 변환합니다(예를 들어 활성화 함수를 사용하지 않는 Dense층이나 합성곱의 특성 맵이라면 활성화 합수가 없는 1x1합성곱).\n",
    "\n",
    "다음 코드는 케라스에서 특성맵의 크기가 같을 때 원본을 그대로 사용하는 잔차 연결을 구현한 예입니다. 여기서는 입력 x가 4D텐서라고 가정합니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "x = Input(shape=(128,128,128))\n",
    "# x에 어떤 변환을 적용합니다.\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# 원본 x를 출력 특성에 더합니다.\n",
    "y = layers.add([y,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 특성 맵의 크기가 다를 때 선형 변환을 사용하여 잔차 연결을 구현한 예입니다(여기에서도 입력 x가 4D텐서라고 가정합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 128, 128, 128 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 128, 128, 128 147584      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 128, 128, 128 147584      conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 128)  16512       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 64, 64, 128)  0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 311,680\n",
      "Trainable params: 311,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "from keras.models import Model\n",
    "\n",
    "x = Input(shape=(128,128,128))\n",
    "# x에 어떤 변환을 적용합니다.\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "# y와 크기를 맞추기 위해 1x1합성곱을 사용하여 원본 텐서 x를 다운샘플링합니다.\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "# 원본 x를 출력 특성에 더합니다.\n",
    "y = layers.add([y,residual])\n",
    "\n",
    "model = Model(x, y)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 층 가중치 공유\n",
    "함수형 API의 중요한 또 하나의 기능은 층 객체를 여러번 재사용 할 수 있다는 것입니다. 층 객체를 두번 호출하면 새로운 층 객체를 만들지 않고 각 호출에 동일한 가중치를 재사용합니다. 이런 기능 때문에 공유 가지를 가진 모델을 만들 수 있습니다. 이런 가지는 같은 가중치를 공유하고 같은 연산을 수행합니다. 다시 말해 같은 표현을 공유하고 이런 표현을 다른 입력에서 합께 학습합니다.\n",
    "\n",
    "다음은 케라스의 함수형 API로 공유 층(재사용 층)을 사용하는 모델을 구현하는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 32)           20608       input_24[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64)           0           lstm_10[0][0]                    \n",
      "                                                                 lstm_10[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            65          concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 20,673\n",
      "Trainable params: 20,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "from keras.models import Model\n",
    "\n",
    "# LSTM객체 하나를 만듭니다\n",
    "lstm = layers.LSTM(32)\n",
    "# 모델의 온쪽 가지를 구성합니다. 입력은 크기가 128인 벡터의 가변 길이 시퀀스입니다.\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "# 모델의 오른쪽 가지를 구성합니다. 기존 층 객체를 호출하면 가중치가 재사용됩니다.\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "# 맨 위에 분류기를 놓습니다.\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model([left_input, right_input], predictions)\n",
    "# model.fit([left_data, right_data], targets)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 층과 모델\n",
    "함수형 API에서는 모델을 층처럼 사용할 수 있습니다. 모델을 '커다란 층'으로 생각해도 됩니다. Sequential 클래스와 Model 클래스에서 모두 동일합니다. 이 말은 입력 텐서로 모델을 호출해서 출력 텐서를 얻을 수 있다는 뜻입니다.\n",
    "```python\n",
    "y = model(x)\n",
    "```\n",
    "모델에서 입력 텐서와 출력 덴서가 여러 개이면 텐서의 리스트로 호출합니다.\n",
    "```python\n",
    "y1, y2 = model([x1,x2])\n",
    "```\n",
    "모델 객체를 호출할 때 모델의 가중치가 재사용됩니다. 층 객체를 호출할 떄와 정확히 같습니다. 층 객체나 모델 객체나 객체를 호풀하는 것은 항상 그 객체가 가진 학습된 표현을 재사용 합니다.\n",
    "\n",
    "모델 객체를 재사용하는 간단한 실전 예는 듀얼 카메라에서 입력을 받는 비전 모델입니다. 두 카메라가 멫 센티미터(1인치) 간격을 두고 나란히 있습니다. 이런 모델은 깊이를 감지할 수 있습니다. 많은 애플리케이션에서 유용한 기능입니다. 왼쪽 카메라와 오른쪽 카메라에서 시각적 특징을 추출하여 합칙기 위해 2개의 독립된 모델을 사용할 필요가 없습니다. 두 입력에 저수준 처리과정이 공유될 수 있습니다. 다시 말해 가중치가 같고 동일한 표현을 공유하는 층을 사용합니다. 다음은 케라스에서 샴 비전 모델(공유 합성곱 기반 층)을 구현하는 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "from keras import applications\n",
    "\n",
    "# 이미지 처리 기본 모델은 엑셉션 네트워크입니다.(합성곱 기반 층만 사용합니다.)\n",
    "xception_base = applications.Xception(weights=None, include_top=False)\n",
    "# 입력은 250x250 RGB 이미지 입니다.\n",
    "left_input = Input(shape=(250,250,3))\n",
    "right_input = Input(shape=(250,250,3))\n",
    "\n",
    "# 같은 비전 모델을 두번 호출합니다.\n",
    "left_features = xception_base(left_input)\n",
    "right_input = xception_base(right_input)\n",
    "\n",
    "# 합쳐진 특성은 오른쪽 입력과 왼쪽 입력에서 얻은 정보를 담고 있습니다.\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것으로 케라스의 함수형 API 소개를 마칩니다. 이는 고급 심층 신경망 구조를 구축하기 위해 필수적인 도구입니다. 여기에서 다음 내용을 배웠습니다.\n",
    "* 차례대로 층을 쌓는 것 이상이 필요할 때는 Sequential API를 사용하지 않습니다.\n",
    "* 함수형 API를 사용하여 다중 입력, 다중 출력, 복잡한 네트워크 토폴로지를 갖는 케라스 모델을 만드는 방법\n",
    "* 다른 네트워크 가지에서 같은 층이나 모델 객체를 여러번 호출하여 가중치를 재사용하는 방법"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
